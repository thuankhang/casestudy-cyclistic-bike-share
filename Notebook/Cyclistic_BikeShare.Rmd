---
title: "Cyclistic Bike Share"
author: "Thuan Khang Dinh"
date: '`r Sys.Date()`'
output: html_document
---

# README:

-   Here is my notebook(from STEP PREPARE to STEP ACT) for all code, and some explanations what I have done.
-   If you want to go to **Github** version follow [link](https://github.com/thuankhang/casestudy-cyclistic-bike-share)
-   If you want to see **Dashboard** follow [link]()



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

required_libs = c("ggplot2","sqldf")
for (l in required_libs) {
  if (!require(l, character.only = TRUE)) {
      install.packages(l)
  }
  library(l, character.only = TRUE)
}
```



# Overview:

In this study, we will get better understanding about the difference of annual members and casual members using bikes. Our objective is to delve into the Cyclistic historical bike trip data, analyzing patterns, preferences, and behaviors of these two distinct groups.

The business goal is to design the strategies to maximizing the number of annual memberships, backed up with compelling data insights and data visualizations, which have been identified that more profitable for Cyclistic.

# STEP 1: ASK

## 1.1 Company backgroud:

Cyclistic is a bike-share company in Chicago, offering a fleet of bicycles and locked into a network of stations across the city. The bikes can be unlocked from one station and returned to any other station in the system anytime. Cyclistic operates in a subscription-based model, with optios of casual members and annual members.

## 1.2 Key stakeholders:

**1. Lily Moreno:** The director of marketing who is responsible for the development of campaigns and initiative to promote the bike-share program.

**2. Cyclistic marketing analytics team:** The team is responsible for collecting, analyzing, and reporting data that helps guide Cyclistic marketing strategy.

**3. Cyclistic executive team:** The team will decide whether to approve the recommended marketing program.

## 1.3 Problem Statement:

Understanding the difference between casual members and annual riders of Cyclistic bikes to design a new marketing strategy of converting casual riders to annual users.

## 1.4 Business Task:

Analyze the Cyclistic’s history bike trip data to understand the differences and use those insights to design a marketing strategy to convert casual to annual riders.

## 1.5 Deliverable:

1.  A clear statement of the business task.
2.  A description of all data sources used.
3.  Documentation of any cleaning or manipulation of data.
4.  A summary of the analysis.
5.  Supporting visualization and key findings.
6.  The top three recommendation based on the analysis.

## 1.6 Data Context:

Our data, derived from Cyclistic's historical trip records, includes a complete year of ride data, providing insights into the usage patterns of both casual and annual members. The crucial variables in our dataset include each trip's start and end times, start and end station details, rideable type, and whether the rider is a casual or annual member.

# STEP 2: PREPARE

## 2.1 Information on Data Source:

1.  In this case study, I am using the Public Bike Trip Data from the [Divvy Bikes website](https://divvybikes.com/system-data): [Datasets from January 2023 to December 2023](https://divvy-tripdata.s3.amazonaws.com/index.html).

2.  The data for this case study is provided by [Motivate International Inc](https://www.divvybikes.com/data-license-agreement), has been made publicly available for download.

3.  The data represent historical trip data of Cyclistic bikes, in format of CSV, with each row is a trip, and columns indicate features such as `trip type`, `start and end time`, `start and end station`, and `user type`.

## 2.2 Limitations of Data Set:

1.  Data-privacy issues prohibit from using riders’ personally identifiable information, so we cannot do biological related analysis.

2.  The data only includes people who have chosen to use the Cyclistic service, and might not represent the entire population of bike riders in Chicago.

## 2.3 Is Data ROCCC:

ROCCC stands for **R**eliable, **O**riginal, **C**omprehensive, **C**urrent, and **C**ited.

1.  Reliable – High – The data is collected and provided by the company that operates the bicycle sharing service in Chicago, under the authority of the City.

2.  Original – High – The data originates directly from Bikeshare’s operations of the Divvy service. It is not derived from secondary sources, so it is authentic and directly related to the service’s operation.

3.  Comprehensive – Moderate – The license grants access to certain Divvy system data, which implies that the data may not be fully comprehensive.

4.  Current – High – It does not provide details on how often the data is updated in license, but if you take a look at the data provided, it does update monthly.

5.  Cited – Low – No explicit citation requirement.

## 2.4 Import Data:

```{r}
# January
tripdata_2023_01 <- read.csv("../Data/2023/202301-divvy-tripdata.csv")


# February
tripdata_2023_02 <- read.csv("../Data/2023/202302-divvy-tripdata.csv")


# March
tripdata_2023_03 <- read.csv("../Data/2023/202303-divvy-tripdata.csv")


# April
tripdata_2023_04 <- read.csv("../Data/2023/202304-divvy-tripdata.csv")


# May
tripdata_2023_05_1 <- read.csv("../Data/2023/202305-divvy-tripdata_1.csv")
tripdata_2023_05_2 <- read.csv("../Data/2023/202305-divvy-tripdata_2.csv")

tripdata_2023_05 = sqldf("
  SELECT * FROM tripdata_2023_05_1
  UNION ALL
  SELECT * FROM tripdata_2023_05_2
")
rm(tripdata_2023_05_1)
rm(tripdata_2023_05_2)


# June
tripdata_2023_06_1 <- read.csv("../Data/2023/202306-divvy-tripdata_1.csv")
tripdata_2023_06_2 <- read.csv("../Data/2023/202306-divvy-tripdata_2.csv")
tripdata_2023_06 = sqldf("
  SELECT * FROM tripdata_2023_06_1
  UNION ALL
  SELECT * FROM tripdata_2023_06_2
")
rm(tripdata_2023_06_1)
rm(tripdata_2023_06_2)


# July
tripdata_2023_07_1 <- read.csv("../Data/2023/202307-divvy-tripdata_1.csv")
tripdata_2023_07_2 <- read.csv("../Data/2023/202307-divvy-tripdata_2.csv")
tripdata_2023_07 = sqldf("
  SELECT * FROM tripdata_2023_07_1
  UNION ALL
  SELECT * FROM tripdata_2023_07_2
")
rm(tripdata_2023_07_1)
rm(tripdata_2023_07_2)


# August
tripdata_2023_08_1 <- read.csv("../Data/2023/202308-divvy-tripdata_1.csv")
tripdata_2023_08_2 <- read.csv("../Data/2023/202308-divvy-tripdata_2.csv")
tripdata_2023_08 = sqldf("
  SELECT * FROM tripdata_2023_08_1
  UNION ALL
  SELECT * FROM tripdata_2023_08_2
")
rm(tripdata_2023_08_1)
rm(tripdata_2023_08_2)


# September
tripdata_2023_09_1 <- read.csv("../Data/2023/202309-divvy-tripdata_1.csv")
tripdata_2023_09_2 <- read.csv("../Data/2023/202309-divvy-tripdata_2.csv")
tripdata_2023_09 = sqldf("
  SELECT * FROM tripdata_2023_09_1
  UNION ALL
  SELECT * FROM tripdata_2023_09_2
")
rm(tripdata_2023_09_1)
rm(tripdata_2023_09_2)


# October
tripdata_2023_10_1 <- read.csv("../Data/2023/202310-divvy-tripdata_1.csv")
tripdata_2023_10_2 <- read.csv("../Data/2023/202310-divvy-tripdata_2.csv")
tripdata_2023_10 = sqldf("
  SELECT * FROM tripdata_2023_10_1
  UNION ALL
  SELECT * FROM tripdata_2023_10_2
")
rm(tripdata_2023_10_1)
rm(tripdata_2023_10_2)


# November
tripdata_2023_11 <- read.csv("../Data/2023/202311-divvy-tripdata.csv")


# December
tripdata_2023_12 <- read.csv("../Data/2023/202312-divvy-tripdata.csv")
```

## 2.5 Understand the data

As all the datasets have the same columns, we will inspect only one of those.

```{r}
str(tripdata_2023_01)
```

```{r}
head(tripdata_2023_01)
```

The dataset is organized as we said, so we are safe to come to next step.

# STEP 3: PROCESS

In this step, I will do several things:

-   Merged all dataframes into one dataframe: using `UNION ALL`
-   Add column `ride_length`: using `julianday(ended_at) - julianday(started_at)` and `AS ride_length`
-   Add column `day_of_week`: using `strftime('%w', started_at) AS day_of_week`
-   Get rid of any observations which have NA value: using `WHERE ... IS NOT NULL`
-   Get rid of case where `started_at` is after `ended_at`: using `datetime(started_at) <= datetime(ended_at)`
-   Change name `started_at` to `start_at` and `ended_at` to `end_at`.

```{r}
df_names = c("tripdata_2023_01", "tripdata_2023_02", "tripdata_2023_03", "tripdata_2023_04", "tripdata_2023_05","tripdata_2023_06", "tripdata_2023_07", "tripdata_2023_08", "tripdata_2023_09", "tripdata_2023_10", "tripdata_2023_11", "tripdata_2023_12")

select_clause = "
    SELECT ride_id, rideable_type, 
          started_at AS start_at, 
          ended_at AS end_at,
          member_casual,
          start_station_name, start_station_id,
          end_station_name, end_station_id,
          CAST((julianday(ended_at) - julianday(started_at)) * 86400 AS INTEGER) AS ride_length, 
          CASE 
            WHEN strftime('%w', started_at) = '0' THEN 'Sunday'
            WHEN strftime('%w', started_at) = '1' THEN 'Monday'
            WHEN strftime('%w', started_at) = '2' THEN 'Tuesday'
            WHEN strftime('%w', started_at) = '3' THEN 'Wednesday'
            WHEN strftime('%w', started_at) = '4' THEN 'Thursday'
            WHEN strftime('%w', started_at) = '5' THEN 'Friday'
            WHEN strftime('%w', started_at) = '6' THEN 'Saturday'
          END AS day_of_week,
          
          CASE 
             WHEN CAST(strftime('%H', started_at) AS INTEGER) BETWEEN 0 AND 5 THEN 'Early Morning'
             WHEN CAST(strftime('%H', started_at) AS INTEGER) BETWEEN 6 AND 8 THEN 'Morning Commute'
             WHEN CAST(strftime('%H', started_at) AS INTEGER) BETWEEN 9 AND 14 THEN 'Midday'
             WHEN CAST(strftime('%H', started_at) AS INTEGER) BETWEEN 15 AND 18 THEN 'Evening Commute'
             WHEN CAST(strftime('%H', started_at) AS INTEGER) BETWEEN 19 AND 23 THEN 'Night'
         END AS time_interval,

         strftime('%m', started_at) AS month"

columns = colnames(tripdata_2023_01)

where_clause = paste0(" WHERE " ,paste(columns, "IS NOT NULL", collapse = " AND "), " AND CAST((julianday(ended_at) - julianday(started_at)) * 86400 AS INTEGER) > 0")

query = paste(select_clause, " FROM ", df_names, collapse = paste0(where_clause," UNION ALL "))
merged_data = sqldf(query)

# Remove df that no longer use for prevent out of memory
rm(tripdata_2023_01)
rm(tripdata_2023_02)
rm(tripdata_2023_03)
rm(tripdata_2023_04)
rm(tripdata_2023_05)
rm(tripdata_2023_06)
rm(tripdata_2023_07)
rm(tripdata_2023_08)
rm(tripdata_2023_09)
rm(tripdata_2023_10)
rm(tripdata_2023_11)
rm(tripdata_2023_12)
  
str(merged_data)
```

# STEP 4: ANALYZE

## Overall:

```{r}
rm(ride_summary)
query = "
SELECT member_casual, 
      COUNT(*) AS number_of_rides,
      SUM(ride_length) AS total_ride_duration,
      AVG(ride_length) AS average_ride_duration,
      MAX(ride_length) AS max_ride_duration,
      MIN(ride_length) AS min_ride_duration
FROM merged_data
GROUP BY member_casual;
"

ride_summary = sqldf(query)
ride_summary
```

## Number of rides:

### By day of week:

#### Table

```{r}
query = "
SELECT member_casual AS user_type, 
       SUM(CASE WHEN day_of_week = 'Sunday' THEN 1 ELSE 0 END) AS Sunday,
       SUM(CASE WHEN day_of_week = 'Monday' THEN 1 ELSE 0 END) AS Monday,
       SUM(CASE WHEN day_of_week = 'Tuesday' THEN 1 ELSE 0 END) AS Tuesday,
       SUM(CASE WHEN day_of_week = 'Wednesday' THEN 1 ELSE 0 END) AS Wednesday,
       SUM(CASE WHEN day_of_week = 'Thursday' THEN 1 ELSE 0 END) AS Thursday,
       SUM(CASE WHEN day_of_week = 'Friday' THEN 1 ELSE 0 END) AS Friday,
       SUM(CASE WHEN day_of_week = 'Saturday' THEN 1 ELSE 0 END) AS Saturday
FROM merged_data
GROUP BY member_casual;
"
rides_by_user_type_day_of_week = sqldf(query)
rides_by_user_type_day_of_week
```

#### Visualize

Prepare dataframe for visualization:

```{r}
query = "
SELECT member_casual AS user_type, day_of_week, COUNT(*) AS num_rides
FROM merged_data
GROUP BY user_type, day_of_week
ORDER BY 
    CASE
        WHEN day_of_week = 'Sunday' THEN 1
        WHEN day_of_week = 'Monday' THEN 2
        WHEN day_of_week = 'Tuesday' THEN 3
        WHEN day_of_week = 'Wednesday' THEN 4
        WHEN day_of_week = 'Thursday' THEN 5
        WHEN day_of_week = 'Friday' THEN 6
        WHEN day_of_week = 'Saturday' THEN 7
    END;
"
df_rides_by_user_type_day_of_week = sqldf(query)
#For order x-axis
df_rides_by_user_type_day_of_week$day_of_week = factor(df_rides_by_user_type_day_of_week$day_of_week, levels=unique(df_rides_by_user_type_day_of_week$day_of_week))

write.csv(df_rides_by_user_type_day_of_week, "../Data/Summary/rides_by_user_type_day_of_week.csv")
```

Pie chart:

```{r}
colors <- c("Monday" = "#FFFAF0",    # Floral White
            "Tuesday" = "#CD5C5C",   # Indian Red
            "Wednesday" = "#32CD32", # Lime Green
            "Thursday" = "#FFFFE0",  # Light Yellow
            "Friday" = "#F0F8FF",    # Alice Blue
            "Saturday" = "#663399",  # Rebecca Purple
            "Sunday" = "#800000")    # Maroon
par(mfrow = c(1,2))
pie(df_rides_by_user_type_day_of_week$num_rides[df_rides_by_user_type_day_of_week$user_type == "casual"],
    df_rides_by_user_type_day_of_week$day_of_week[df_rides_by_user_type_day_of_week$user_type == "casual"],
    sub = "Casual User",
    col = colors[df_rides_by_user_type_day_of_week$day_of_week[df_rides_by_user_type_day_of_week$user_type == "casual"]])

pie(df_rides_by_user_type_day_of_week$num_rides[df_rides_by_user_type_day_of_week$user_type == "member"],
    df_rides_by_user_type_day_of_week$day_of_week[df_rides_by_user_type_day_of_week$user_type == "member"],
    sub = "Member User",
    col = colors[df_rides_by_user_type_day_of_week$day_of_week[df_rides_by_user_type_day_of_week$user_type == "member"]])
title("Rides Distribution by Day of the Week",line = - 2, outer = TRUE)
```

Multiple Bar Plot

```{r}
ggplot(df_rides_by_user_type_day_of_week, aes(x = day_of_week, y = num_rides, fill = user_type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Number of Rides by User Type and Day of the Week",
       x = "Day of the Week",
       y = "Number of Rides") +
  theme_minimal() +
  scale_y_continuous(labels = scales::label_number())
```

**Insight**

-   Casual users primarily use the service on weekends, with a peak on Saturday, indicating more leisure or occasional usage.
-   Members use the service more consistently throughout the week, with higher usage on weekdays, particularly on Thursday, suggesting more regular or commuter-based usage.

### By time interval:

Let define something first:

-   I will use start time only, because it provides a string indication of when users decide to take a trip.
-   I will split the day into 5 intervals that align with potential user behavior:
    -   **Early Morning (12:00 AM - 6:00 AM):** Night-shift workers, late-night leisure riders, or early commutes.
    -   **Morning Commute (6:00 AM - 9:00 AM):** Typical morning rush hours.
    -   **Midday (9:00 AM - 3:00 PM):** More relaxed period, include leisure rides, running errands, or flexible work schedules.
    -   **Evening Commute (3:00 PM - 7:00 PM):** Peak usage time as people return home form work or school.
    -   **Night (7:00 PM - 12:00 AM):** Leisure, social outings, or exercise.

#### Table

```{r}
query = "
SELECT member_casual AS user_type, 
       SUM(CASE WHEN time_interval = 'Early Morning' THEN 1 ELSE 0 END) AS EarlyMorning,
       SUM(CASE WHEN time_interval = 'Morning Commute' THEN 1 ELSE 0 END) AS MorningCommute,
       SUM(CASE WHEN time_interval = 'Midday' THEN 1 ELSE 0 END) AS Midday,
       SUM(CASE WHEN time_interval = 'Evening Commute' THEN 1 ELSE 0 END) AS EveningCommute,
       SUM(CASE WHEN time_interval = 'Night' THEN 1 ELSE 0 END) AS Night
FROM merged_data
GROUP BY member_casual;
"
rides_by_user_type_time_interval = sqldf(query)
rides_by_user_type_time_interval
```

#### Visualize

Prepare dataframe for visualization:

```{r}
query = "
SELECT member_casual AS user_type, time_interval, COUNT(*) AS num_rides
FROM merged_data
GROUP BY user_type, time_interval
ORDER BY 
    CASE
        WHEN time_interval = 'Early Morning' THEN 1
        WHEN time_interval = 'Morning Commute' THEN 2
        WHEN time_interval = 'Midday' THEN 3
        WHEN time_interval = 'Evening Commute' THEN 4
        WHEN time_interval = 'Night' THEN 5
    END;
"
df_rides_by_user_type_time_interval = sqldf(query)
#For order x-axis
df_rides_by_user_type_time_interval$time_interval = factor(df_rides_by_user_type_time_interval$time_interval, levels=unique(df_rides_by_user_type_time_interval$time_interval))

write.csv(df_rides_by_user_type_time_interval, "../Data/Summary/df_rides_by_user_type_time_interval.csv")
```

Pie chart:

```{r}
# Define colors for the 5 time intervals
time_interval_colors <- c("Early Morning" = "#ADD8E6",  # Light Blue
                          "Morning Commute" = "#FFFF00", # Yellow
                          "Midday" = "#32CD32",          # Lime Green
                          "Evening Commute" = "#FFA500", # Orange
                          "Night" = "#00008B")           # Dark Blue

par(mfrow = c(1,2), mar = c(5, 5, 5, 5))
pie(df_rides_by_user_type_time_interval$num_rides[df_rides_by_user_type_time_interval$user_type == "casual"],
    df_rides_by_user_type_time_interval$time_interval[df_rides_by_user_type_time_interval$user_type == "casual"],
    sub = "Casual User",
    col = time_interval_colors[df_rides_by_user_type_time_interval$time_interval[df_rides_by_user_type_time_interval$user_type == "casual"]])

pie(df_rides_by_user_type_time_interval$num_rides[df_rides_by_user_type_time_interval$user_type == "member"],
    df_rides_by_user_type_time_interval$time_interval[df_rides_by_user_type_time_interval$user_type == "member"],
    sub = "Member User",
    col = time_interval_colors[df_rides_by_user_type_time_interval$time_interval[df_rides_by_user_type_time_interval$user_type == "member"]])
title("Rides Distribution by Time interval",line = - 2, outer = TRUE)
```

Multiple Bar Plot

```{r}
ggplot(df_rides_by_user_type_time_interval, aes(x = time_interval, y = num_rides, fill = user_type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Number of Rides by User Type and Time Interval",
       x = "Time Interval",
       y = "Number of Rides") +
  theme_minimal() +
  scale_y_continuous(labels = scales::label_number())
```

**Insight**

-   Casual Users: Peak usage during midday and evening commute; active at night, indicating flexible, leisure-oriented rides rather than structured commuting.
-   Member Users: Dominant during commuting hours, with consistent usage across all intervals, reflecting structured, regular travel patterns.

### By month:

#### Table

```{r}
query = "
SELECT member_casual AS user_type, 
       SUM(CASE WHEN month = '01' THEN 1 ELSE 0 END) AS Jan,
       SUM(CASE WHEN month = '02' THEN 1 ELSE 0 END) AS Feb,
       SUM(CASE WHEN month = '03' THEN 1 ELSE 0 END) AS Mar,
       SUM(CASE WHEN month = '04' THEN 1 ELSE 0 END) AS Apr,
       SUM(CASE WHEN month = '05' THEN 1 ELSE 0 END) AS May,
       SUM(CASE WHEN month = '06' THEN 1 ELSE 0 END) AS Jun,
       SUM(CASE WHEN month = '07' THEN 1 ELSE 0 END) AS Jul,
       SUM(CASE WHEN month = '08' THEN 1 ELSE 0 END) AS Aug,
       SUM(CASE WHEN month = '09' THEN 1 ELSE 0 END) AS Sep,
       SUM(CASE WHEN month = '10' THEN 1 ELSE 0 END) AS Oct,
       SUM(CASE WHEN month = '11' THEN 1 ELSE 0 END) AS Nov,
       SUM(CASE WHEN month = '12' THEN 1 ELSE 0 END) AS Dec
FROM merged_data
GROUP BY member_casual;
"
rides_by_user_type_month = sqldf(query)
rides_by_user_type_month
```

#### Visualize

Prepare dataframe for visualization:

```{r}
query = "
SELECT member_casual AS user_type, month, COUNT(*) AS num_rides
FROM merged_data
GROUP BY user_type, month
ORDER BY 
    CASE
        WHEN month = '01' THEN 1
        WHEN month = '02' THEN 2
        WHEN month = '03' THEN 3
        WHEN month = '04' THEN 4
        WHEN month = '05' THEN 5
        WHEN month = '06' THEN 6
        WHEN month = '07' THEN 7
        WHEN month = '08' THEN 8
        WHEN month = '09' THEN 9
        WHEN month = '10' THEN 10
        WHEN month = '11' THEN 11
        WHEN month = '12' THEN 12
    END;
"
df_rides_by_user_type_month = sqldf(query)
#For order x-axis
df_rides_by_user_type_month$month = factor(df_rides_by_user_type_month$month, levels=unique(df_rides_by_user_type_month$month))

write.csv(df_rides_by_user_type_month, "../Data/Summary/df_rides_by_user_type_month.csv")
```

Pie chart:

```{r}
par(mfrow = c(1,2))
pie(df_rides_by_user_type_month$num_rides[df_rides_by_user_type_month$user_type == "casual"],
    df_rides_by_user_type_month$month[df_rides_by_user_type_month$user_type == "casual"],
    sub = "Casual User")

pie(df_rides_by_user_type_month$num_rides[df_rides_by_user_type_month$user_type == "member"],
    df_rides_by_user_type_month$month[df_rides_by_user_type_month$user_type == "member"],
    sub = "Member User")
title("Rides Distribution by Month",line = - 2, outer = TRUE)
```

Multiple Bar Plot

```{r}
ggplot(df_rides_by_user_type_month, aes(x = month, y = num_rides, fill = user_type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Number of Rides by User Type and Month",
       x = "Time Interval",
       y = "Number of Rides") +
  theme_minimal() +
  scale_y_continuous(labels = scales::label_number())
```


**Insight:**

-   Casual Users: Their rides peak during the summer months, strongly indicating that their primary use is for leisure or recreational purposes. This behavior is heavily influenced by warm weather, aligning with seasonal outdoor activities.
-   Member Users: Their rides show a more consistent pattern throughout the year, with less dramatic seasonal variation compared to casual users. This suggests that members likely use bikes not just for recreation, but also for commuting or other routine activities, making their usage more practical and need-based.


## Ride length (ride duration):
